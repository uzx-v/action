#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import hashlib
import base64

import cloudscraper
from bs4 import BeautifulSoup
import cv2
import requests

# ==== é…ç½® ====
BRIGHTNESS_THRESHOLD = 130
BATCH_SIZE = 100
TEMP_DIR = "temp_download"

# èµ·å§‹é…ç½®
BASE_URL = "https://img.hyun.cc/index.php/archives/"
START_ID = 342

# ç›®æ ‡ç§æœ‰ä»“åº“
TARGET_REPO = os.environ.get("TARGET_REPO", "")  # æ ¼å¼: owner/repo
GITHUB_TOKEN = os.environ.get("GH_TOKEN", "")
TARGET_BRANCH = "main"

# ç›®æ ‡ä»“åº“ä¸­çš„è·¯å¾„
IMAGES_DIR = "ri"
FOLDERS = ["vd", "vl", "hd", "hl"]

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)


# ============ GitHub API ============

def github_get_file(path: str) -> tuple:
    """è·å–ç›®æ ‡ä»“åº“ä¸­çš„æ–‡ä»¶å†…å®¹å’ŒSHA"""
    if not GITHUB_TOKEN or not TARGET_REPO:
        return None, None
    
    url = f"https://api.github.com/repos/{TARGET_REPO}/contents/{path}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            content = base64.b64decode(data["content"]).decode("utf-8")
            return content, data["sha"]
    except Exception as e:
        print(f"âš ï¸ è·å–æ–‡ä»¶å¤±è´¥ {path}: {e}")
    return None, None


def github_upload(path: str, content: bytes, message: str) -> bool:
    """ä¸Šä¼ æ–‡ä»¶åˆ°ç›®æ ‡ä»“åº“"""
    if not GITHUB_TOKEN or not TARGET_REPO:
        print("âŒ ç¼ºå°‘ GITHUB_TOKEN æˆ– TARGET_REPO")
        return False
    
    url = f"https://api.github.com/repos/{TARGET_REPO}/contents/{path}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    # è·å–ç°æœ‰æ–‡ä»¶çš„SHAï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    _, sha = github_get_file(path)
    
    data = {
        "message": message,
        "content": base64.b64encode(content).decode("utf-8"),
        "branch": TARGET_BRANCH
    }
    if sha:
        data["sha"] = sha
    
    try:
        resp = requests.put(url, headers=headers, json=data, timeout=60)
        return resp.status_code in [200, 201]
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥ {path}: {e}")
        return False


def get_remote_json(path: str, default=None) -> dict:
    """ä»ç›®æ ‡ä»“åº“è·å–JSONæ–‡ä»¶"""
    content, _ = github_get_file(path)
    if content:
        try:
            return json.loads(content)
        except:
            pass
    return default if default is not None else {}


def save_remote_json(path: str, data: dict, msg: str) -> bool:
    """ä¿å­˜JSONåˆ°ç›®æ ‡ä»“åº“"""
    content = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
    return github_upload(path, content, msg)


# ============ å·¥å…·å‡½æ•° ============

def get_file_hash(filepath: str) -> str:
    sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()


# ============ å›¾ç‰‡å¤„ç† ============

def scrape_images(url: str) -> list:
    """çˆ¬å–é¡µé¢ä¸­çš„å›¾ç‰‡é“¾æ¥"""
    print(f"ğŸŒ çˆ¬å–: {url}")
    
    try:
        resp = scraper.get(url, timeout=30)
        resp.raise_for_status()
        resp.encoding = 'utf-8'
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "lxml")
    images = []
    
    for idx, link in enumerate(soup.find_all("a", {"data-fancybox": True}), 1):
        href = link.get("href", "")
        if href.startswith("http"):
            images.append({"url": href, "index": idx})
    
    print(f"ğŸ“· æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
    return images


def download_image(url: str, save_path: str) -> bool:
    try:
        resp = scraper.get(url, timeout=60, stream=True)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def convert_to_webp(input_path: str, output_path: str) -> bool:
    try:
        img = cv2.imread(input_path)
        if img is None:
            return False
        cv2.imwrite(output_path, img, [cv2.IMWRITE_WEBP_QUALITY, 85])
        return True
    except:
        return False


def analyze_image(path: str) -> dict | None:
    """åˆ†æå›¾ç‰‡ï¼Œè¿”å›åˆ†ç±»æ–‡ä»¶å¤¹"""
    try:
        img = cv2.imread(path)
        if img is None:
            return None
        
        h, w = img.shape[:2]
        if w < 10 or h < 10:
            return None
        
        orientation = "h" if w >= h else "v"
        
        resized = cv2.resize(img, (100, 100))
        lab = cv2.cvtColor(resized, cv2.COLOR_BGR2LAB)
        avg_l = lab[:, :, 0].mean()
        brightness = "d" if avg_l < BRIGHTNESS_THRESHOLD else "l"
        
        folder = orientation + brightness
        print(f"  ğŸ“ {w}x{h} L={avg_l:.1f} â†’ {folder}")
        
        return {"folder": folder}
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None


# ============ é¡µé¢å¤„ç† ============

def process_page(page_id: int) -> str:
    """
    å¤„ç†å•ä¸ªé¡µé¢
    è¿”å›: "success" | "empty" | "error"
    """
    url = f"{BASE_URL}{page_id}.html"
    
    print(f"\n{'='*50}")
    print(f"ğŸ“‚ å¤„ç†é¡µé¢ ID: {page_id}")
    print(f"ğŸ”— {url}")
    print(f"{'='*50}\n")
    
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # çˆ¬å–å›¾ç‰‡
    images = scrape_images(url)
    if not images:
        return "empty"
    
    # è·å–è¿œç¨‹æ•°æ®
    hash_registry = get_remote_json(f"{IMAGES_DIR}/hash_registry.json", {})
    folder_counts = get_remote_json(f"{IMAGES_DIR}/count.json", {})
    
    for f in FOLDERS:
        if f not in folder_counts:
            folder_counts[f] = 0
    
    new_count = 0
    
    for img in images[:BATCH_SIZE]:
        idx = img["index"]
        temp_path = os.path.join(TEMP_DIR, f"temp_{idx}")
        webp_path = os.path.join(TEMP_DIR, f"temp_{idx}.webp")
        
        print(f"\nğŸ“¥ [{idx}/{len(images)}] ä¸‹è½½ä¸­...")
        
        if not download_image(img["url"], temp_path):
            continue
        
        # æ£€æŸ¥é‡å¤
        file_hash = get_file_hash(temp_path)
        if file_hash in hash_registry:
            print(f"  â­ï¸ è·³è¿‡é‡å¤")
            os.remove(temp_path)
            continue
        
        # åˆ†æå›¾ç‰‡
        info = analyze_image(temp_path)
        if not info:
            os.remove(temp_path)
            continue
        
        # è½¬æ¢æ ¼å¼
        if not convert_to_webp(temp_path, webp_path):
            os.remove(temp_path)
            continue
        os.remove(temp_path)
        
        # ç¡®å®šç›®æ ‡è·¯å¾„
        target_folder = info["folder"]
        folder_counts[target_folder] += 1
        new_num = folder_counts[target_folder]
        remote_path = f"{IMAGES_DIR}/{target_folder}/{new_num}.webp"
        
        # ä¸Šä¼ å›¾ç‰‡åˆ°ç›®æ ‡ä»“åº“
        with open(webp_path, "rb") as f:
            webp_data = f.read()
        
        if github_upload(remote_path, webp_data, f"Add {target_folder}/{new_num}.webp"):
            hash_registry[file_hash] = f"{target_folder}/{new_num}.webp"
            new_count += 1
            print(f"  âœ… ä¸Šä¼ : {remote_path}")
        else:
            folder_counts[target_folder] -= 1
            print(f"  âŒ ä¸Šä¼ å¤±è´¥")
        
        os.remove(webp_path)
    
    # ä¿å­˜å…ƒæ•°æ®åˆ°ç›®æ ‡ä»“åº“
    if new_count > 0:
        save_remote_json(
            f"{IMAGES_DIR}/hash_registry.json", 
            hash_registry, 
            f"Update hash_registry (page {page_id})"
        )
        save_remote_json(
            f"{IMAGES_DIR}/count.json", 
            folder_counts, 
            f"Update count (page {page_id})"
        )
        print(f"\nğŸ’¾ å·²æ›´æ–° count.json å’Œ hash_registry.json")
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    if os.path.exists(TEMP_DIR):
        for f in os.listdir(TEMP_DIR):
            os.remove(os.path.join(TEMP_DIR, f))
        os.rmdir(TEMP_DIR)
    
    print(f"\nâœ… é¡µé¢ {page_id} å®Œæˆï¼Œæ–°å¢ {new_count} å¼ ")
    return "success"


# ============ ä¸»å‡½æ•° ============

def main():
    print("ğŸš€ å¼€å§‹è¿è¡Œ\n")
    
    # æ£€æŸ¥é…ç½®
    if not GITHUB_TOKEN:
        print("âŒ ç¼ºå°‘ GH_TOKEN ç¯å¢ƒå˜é‡")
        return
    if not TARGET_REPO:
        print("âŒ ç¼ºå°‘ TARGET_REPO ç¯å¢ƒå˜é‡")
        return
    
    print(f"ğŸ“¦ ç›®æ ‡ä»“åº“: {TARGET_REPO}")
    print(f"ğŸ“ å­˜å‚¨ç›®å½•: /{IMAGES_DIR}/")
    
    # ä»ç›®æ ‡ä»“åº“è¯»å–è¿›åº¦
    progress = get_remote_json("progress.json", {"last_success_id": START_ID - 1})
    current_id = progress.get("last_success_id", START_ID - 1) + 1
    
    print(f"ğŸ“ å½“å‰è¿›åº¦: ä» ID {current_id} å¼€å§‹\n")
    
    # å¾ªç¯å¤„ç†
    while True:
        result = process_page(current_id)
        
        if result == "success":
            # âœ… æˆåŠŸï¼Œä¿å­˜è¿›åº¦åˆ°ç›®æ ‡ä»“åº“ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
            progress["last_success_id"] = current_id
            save_remote_json("progress.json", progress, f"Update progress to {current_id}")
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {current_id}\n")
            current_id += 1
            
        elif result == "empty":
            # â¹ï¸ æ²¡æœ‰å›¾ç‰‡ï¼Œåœæ­¢æ‰§è¡Œ
            print(f"\nâ¹ï¸ é¡µé¢ {current_id} æ²¡æœ‰å›¾ç‰‡ï¼Œåœæ­¢æ‰§è¡Œ")
            print(f"ğŸ’¡ ä¸‹æ¬¡è¿è¡Œå°†ç»§ç»­å°è¯• ID {current_id}")
            break
            
        else:
            # âŒ å‡ºé”™ï¼Œåœæ­¢
            print(f"\nâŒ å¤„ç†å‡ºé”™ï¼Œåœæ­¢")
            break
    
    print("\nğŸ è¿è¡Œç»“æŸ")


if __name__ == "__main__":
    main()
